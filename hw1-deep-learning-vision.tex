\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
\usepackage[parfill]{parskip}    			% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{enumerate}
\usepackage{tikz}


\title{Homework 1 \\ IMGS 789 Deep Learning for Vision Fall 2016}
%\author{John Doe - \texttt{jdoe@rit.edu}}
\date{}					

\begin{document}

\maketitle

\textbf{Due: 9:00 PM EDT, September 30, 2016}

\section*{Instructions}

Your homework submission must cite any references used (including articles, books, code, websites, and personal communications).  All solutions must be written in your own words, and you must program the algorithms yourself. \textbf{If you do work with others, you must list the people you worked with.} Submit your solutions as a PDF to the Dropbox Folder on MyCourses.

Your homework solution must be prepared in \LaTeX \, and output to PDF format. I suggest using \url{http://overleaf.com} or BaKoMa \TeX \,  to create your document. Overleaf is free and can be accessed online.

Your programs must be written in either MATLAB or Python. The relevant code to the problem should be in the PDF you turn in. If a problem involves programming, then the code should be shown as part of the solution to that problem. One easy way to do this in \LaTeX \, is to use the verbatim environment, i.e., \textbackslash begin\{verbatim\} YOUR CODE \textbackslash end\{verbatim\}

If you have forgotten your linear algebra, you may find  \textit{The Matrix Cookbook} useful, which can be readily found online. You may wish to use the program \textit{MathType}, which can easily export equations to AMS \LaTeX \, so that you don't have to write the equations in \LaTeX \, directly: \url{http://www.dessci.com/en/products/mathtype/}

\sloppy
\textbf{If told to implement an algorithm, don't use a toolbox, or you will receive no credit.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Problem 1 - Softmax Properties}

\subsection*{Part 1 (7 points)}
Recall the softmax function, which is the most common activation function used for the output of a neural network trained to do classification. In a vectorized form, it is given by
\begin{equation*}
\operatorname{softmax} \left( {\mathbf{a}} \right) = \frac{{\exp \left( {\mathbf{a}} \right)}}
{{\sum\nolimits_{j = 1}^K {\exp \left( {a_j } \right)} }},
\end{equation*}
where $\mathbf{a}\in \mathbb{R}^K$. The $\exp$ function in the numerator is applied element-wise and $a_j$ denotes the $j$'th element of $\mathbf{a}$.

Show that the softmax function is invariant to constant offsets to its input, i.e., 
\[
\operatorname{softmax} \left( {{\mathbf{a}} + {c\mathbf{1}}} \right) = \operatorname{softmax} \left( {\mathbf{a}} \right),
\]
where $c\in \mathbb{R}$ is some constant and $\mathbf{1}$ denotes a column vector of 1's.

\textbf{Solution:} \\

%----------------------------------
\subsection*{Part 2 (3 points)}
In practice, why is the observation that the softmax function is invariant to constant offsets to its input important when implementing it in a neural network?

\textbf{Solution:} \\

%---------------------------------
\section*{Problem 2 - Implementing a Softmax Classifier}

For this problem, you will use the 2-dimensional Iris dataset. Download \texttt{iris-train.txt} and \texttt{iris-test.txt} from MyCourses. Each row is one data instance. The first column is the label (1, 2 or 3) and the next two columns are features.

\subsection*{Part 1 - Implementation \& Evaluation (30 points)}
Recall that a softmax classifier is a shallow one-layer neural network of the form:
\begin{equation*}
P\left( {C = k|{\bf{x}}} \right) = \frac{{\exp \left( {{\bf{w}}_k^T {\bf{x}}} \right)}}
{{\sum\nolimits_{j = 1}^K {\exp \left( {{\bf{w}}_j^T {\bf{x}}} \right)} }}
\end{equation*}
where $\mathbf{x}$ is the vector of inputs, $K$ is the total number of categories, and $\mathbf{w}_k$ is the weight vector for category $k$.

In this problem you will implement a softmax classifier from scratch. \textbf{Do not use a toolbox.} Use the softmax (cross-entropy) loss with $L_2$ weight decay regularization. Your implementation should use stochastic gradient descent with mini-batches and momentum to minimize softmax (cross-entropy) loss of this single layer neural network. To make your implementation fast, do as much as possible using matrix and vector operations. This will allow your code to use your environment's BLAS. Your code should loop over epochs and mini-batches, but do not iterate over individual elements of vectors and matrices. Try to make your code as fast as possible. I suggest using profiling and timing tools to do this.

Train your classifier on the Iris dataset for 1000 epochs. Hand tune the hyperparameters (i.e., learning rate, mini-batch size, momentum rate, and $L_2$ weight decay factor) to achieve the best possible training accuracy. During a training epoch, your code should compute the mean per-class accuracy for the training data and the loss. After each epoch, compute the mean per-class accuracy for the testing data and the loss as well. \textbf{The test data should not be used for updating the weights.}

After you have tuned the hyperparameters, generate two plots next to each other. The one on the left should show the cross-entropy loss during training for both the train and test sets as a function of the number of training epochs. The plot on the right should show the mean per-class accuracy as a function of the number of training epochs on both the train set and the test set. 

What is the best test accuracy your model achieved? What hyperparameters did you use? Would early stopping have helped improve accuracy on the test data?

\textbf{Solution:} \\



%--------------------
\subsection*{Part 2 - Displaying Decision Boundaries (10 points)}

Plot the decision boundaries learned by softmax classifier on the Iris dataset, just like we saw in class. On top of the decision boundaries, generate a scatter plot of the training data. Make sure to label the categories.

\textbf{Solution:} \\


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Problem 3 - Classifying Images}

The CIFAR-10 dataset contains 60,000 RGB images from 10 categories. Download it from here: \url{https://www.cs.toronto.edu/~kriz/cifar.html}\\
Read the documentation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{Part 1 (10 points)}

Using the first CIFAR-10 training batch file, display the first three images from each of the 10 categories as a $3 \times 10$ image array. The images are stored as rows, and you will need to reshape them into $32 \times 32 \times 3$ images. 

\textbf{Solution:}\\

%-----------
\subsection*{Part 2 (20 points)}
Using the softmax classifier you implemented, train the model on CIFAR-10's training partitions. To do this, you will need to treat each image as a vector. You will need to tweak the hyperparmaters you used earlier. 

Plot the training loss as a function of training epochs. Try to minimize the error as much as possible. What were the best hyperparmeters? Output the final test accuracy and a normalized $10 \times 10$ confusion matrix computed on the test partition. Make sure to label the columns and rows of the confusion matrix.

\textbf{Solution:}\\


%---------------
\section*{Softmax Classifier Code Appendix}


\end{document}  